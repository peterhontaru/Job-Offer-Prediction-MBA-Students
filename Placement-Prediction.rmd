---
    output:
      html_document:
              
        toc: true
        toc_float: false
        toc_depth: 3
        number_sections: true
        
        code_folding: hide
        code_download: true
        
        fig_width: 9 
        fig_height: 4
        fig_align: "center"
        
        highlight: pygments
        theme: cerulean
        
        keep_md: true
        
    title: "Job Offer Prediction - MBA Students"
    subtitle: "I'm... hired? Can we predict which student received a placement offer?"
    author: "by Peter Hontaru"
---


```{r}
knitr::opts_chunk$set(
    echo = TRUE, # show all code
    tidy = FALSE, # cleaner code printing
    size = "small", # smaller code
    
    fig.path = "figures/", #graphics location
    out.width = "100%",

    message = FALSE,
    warning = FALSE
    )
```


```{r Import Libraries, echo=FALSE}
#import libraries
library(ggplot2)
library(tidyverse) #readr, dplyr, purrr
library(lubridate)
library(readr)
library(data.table)
library(kableExtra)
library(quantreg) #for weighted boxplots
library(scales) #lautomatically determine breaks and labels for axes/legends
library(ggcorrplot)
library(zoo) #get the month of a date
library(caret)
library(doSNOW) #allows for training in paralel
library(ggpubr) #publication plots
library(rstatix) #t-tests
library(ggthemes)
```


```{r}
setwd("~/DS/AHT/Data")

#import data
raw_data <- read_csv("~/DS/Job-Offer-Prediction-MBA-Students/raw data/Placement_Data_Full_Class.csv")%>%
  mutate(gender = as.factor(gender),
               ssc_b = as.factor(ssc_b),
               hsc_b = as.factor(hsc_b),
               hsc_s = as.factor(hsc_s),
               degree_t = as.factor(make.names(degree_t)),
               specialisation = as.factor(make.names(specialisation)),
               workex = as.factor(workex),
               status = as.factor(status))
```


# Introduction


## Problem Statement:

Can we predict which candiates was placed in a role? If so, which factors helped the most (ie. work experience, degree, school results, gender, etc)?

## Key takeaways


#### Students {-}

The LDA2 model might be more suitable as it did not have any false positives. This means that it correctly predicted if a student shouldn't apply for a role. 

The most important factors, as shown by the Random Forrest model were secondary scores, higher secondary schores and work experience.


#### Hiring Managers {-}

The second model might be more suitable as it achieved a higher overall accuracy and more student were predicted correctly.


The model achieved 


## Dataset information:


```{r}
attribute <- names(raw_data)

description <- c(
"Serial Number",
"Gender: Male='M', Female='F'",
"Secondary Education Percentage (grades 9 and 10) - exam at the end of 10th grade",
"Board of Education - Central/Others",
"Higher Secondary Education (grades 11 and 12) - exam at the end of 12th grade",
"Board of Education - Central/Others",
"Specialization in Higher Secondary Education",
"Degree Percentage",
"Undergraduate (Degree type) - Field of degree education",
"Work Experience - Yes/No",
"Employability test Percentage (conducted by college)",
"Post Graduation (MBA) - Specialization",
"MBA percentage",
"Status of placement - Placed/Not placed",
"Salary offered by corporate to candidates")

type <- c(
"numeric",
"factor",
"numeric",
"factor",
"numeric",
"factor",
"factor",
"numeric",
"factor",
"factor",
"numeric",
"factor",
"numeric",
"factor",
"numeric")

dataset_table <-as.data.frame(cbind(attribute, type, description))

dataset_table %>%
  kbl(align = c("r", "l", "l"))%>%
  kable_paper("hover", full_width = F)%>%
  column_spec(1, bold = TRUE)%>%
  column_spec(2, italic = TRUE)

#clear variables from environment
rm(attribute, type, description, dataset_table)
```


Check for *NULL*s:


```{r}
#check for nulls
raw_data %>% summarise_all(~ sum(is.null(.))) %>% sum() %>% 
  kbl(col.names = "Number of NULLs",
      align = c("c", "c")) %>%
  kable_paper("hover", full_width = F)%>%
  column_spec(1, color = "green", bold = TRUE)
```


Check for *NA*s:


```{r}
#check NAs
raw_data %>% summarise_all(~ sum(is.na(.))) %>% sum() %>%
    kbl(col.names = "Number of NAs",
      align = c("c", "c")) %>%
  kable_paper("hover", full_width = F)%>%
  column_spec(1, color = "red", bold = TRUE)
```


Let's dig deeper and see why we have 67 NAs.


```{r}
#we have 67 NAs - let's see which category has these values
raw_data %>% summarise_all(~ sum(is.na(.))) %>%
    kbl()%>%
  kable_paper("hover", full_width = F)%>%
  column_spec(15, color = "red", bold = TRUE)%>%
  column_spec(1:14, color = "green", bold = TRUE)
```


Lastly, we need to check why we have these 67 NAs in the salary category. Is this missing data or another reason behind it? 


```{r}
#let's check that this is because some students did not get placed and thus, had no salary
raw_data %>% group_by(status) %>% count() %>%     
  kbl(col.names = c("Status", "n"),
      align = c("c", "c")) %>%
  kable_paper("hover", full_width = F)%>%
  row_spec(1, color = "red", bold = TRUE)%>%
  row_spec(2, color = "green", bold = TRUE)
```


It looks like we have 67 NAs in the salary column due to the fact that 67 students did not get a placement. That makes sense and no further investigation is needed.


# Exploratory Data Analysis (EDA)


## Correlation plot


**Key findings**:

* we can observe **medium correlations between the academic scores**. This suggests that students who performed well in secondary school were likely to also perform well within further education (higher secondary, university and MBA)
* interestingly, **employability test scores only had a low correlation with academic scores**. **Perhaps**, this suggests that these tests were more practical than theoretical


```{r}
#select all to start
raw_data_corr <- select_if(raw_data, is.numeric)%>%
  select(-sl_no, -salary)

# Compute a correlation matrix
corr <- round(cor(raw_data_corr),2)

# Compute a matrix of correlation p-values
p.mat <- cor_pmat(raw_data_corr)

# Visualize the correlation matrix
ggcorrplot(corr, method = "square", 
           ggtheme = ggthemes::theme_few, 
           
           outline.col = "black",
           colors = c("#00AFBB","white", "red"),
           
           lab = TRUE,
           lab_size = 5,
           digits = 2,
           
           type = "lower",
           legend = "",
           tl.cex = 12,
           
           title = "We can observe low to medium correlations \nbetween our main variables")
           
#clear variables from the environment
rm(corr, p.mat, raw_data_corr)
```


## Grade distribution {.tabset .tabset-fade .tabset-pills}


**Key findings**:

* the distribution becomes more concentated around the median (60-70) as a student progresses in their education, **from secondary** (wide distribution) **to MBA** (narrow distribution)
* the **employability test** has a different trend, with a very wide and almost equal distribution of each score group


### secondary {-}


```{r}
raw_data %>%
  ggplot(aes(ssc_p))+
  geom_histogram(binwidth = 5, fill = "#00AFBB", col = "black")+
  coord_cartesian(xlim=c(30,100),
                  ylim=c(0,70))+
  labs(x = "Score",
       y = "Number of students")+
  theme_few()
```


### higher secondary {-}


```{r}
raw_data %>%
  ggplot(aes(hsc_p))+
  geom_histogram(binwidth = 5, fill = "#00AFBB", col = "black")+
  coord_cartesian(xlim=c(30,100),
                  ylim=c(0,70))+
  labs(x = "Score",
       y = "Number of students")+
  theme_few()
```


### university {-}


```{r}
raw_data %>%
  ggplot(aes(degree_p))+
  geom_histogram(binwidth = 5, fill = "#00AFBB", col = "black")+
  coord_cartesian(xlim=c(30,100),
                  ylim=c(0,70))+
  labs(x = "Score",
       y = "Number of students")+
  theme_few()
```


### MBA {-}


```{r}
raw_data %>%
  ggplot(aes(mba_p))+
  geom_histogram(binwidth = 5, fill = "#00AFBB", col = "black")+
  coord_cartesian(xlim=c(30,100),
                  ylim=c(0,70))+
  labs(x = "Score",
       y = "Number of students")+
  theme_few()
```


### employability (non-academic) {-}


```{r}
raw_data %>%
  ggplot(aes(etest_p))+
  geom_histogram(binwidth = 5, fill = "#E69F00", col = "black")+
  coord_cartesian(xlim=c(30,100),
                  ylim=c(0,70))+
  labs(x = "Score",
       y = "Number of Students")+
  theme_few()
```


## Are there any gender-specific differences in performance scores? {.tabset .tabset-fade .tabset-pills}


**Key findings**:

* females scored significantly higher than men at **university** and **MBA** level
* no significant differences in performance during **secondary**, **higher secondary** and **employability test**


### university {-}


```{r}
t_test_degree <- raw_data%>%
  t_test(degree_p ~ gender)%>%
  add_significance()

raw_data %>% ggplot(aes(gender, degree_p, fill = gender))+
        geom_boxplot(varwidth = TRUE)+
  scale_fill_manual(values = c("#00AFBB", "#E69F00"))+
  labs(title = paste("Females scored significantly higher (", t_test_degree$p.signif, ") than males at the university level"),
       fill = "Gender",
       x = NULL,
       y = "Score")+
  theme_few()

t_test_degree%>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```


### MBA {-}


```{r}
t_test_mba <- raw_data%>%
  t_test(mba_p ~ gender)%>%
  add_significance()

raw_data %>% ggplot(aes(gender, mba_p, fill = gender))+
        geom_boxplot(varwidth = TRUE)+
  scale_fill_manual(values = c("#00AFBB", "#E69F00"))+
  labs(title = paste("Females scored significantly higher (", t_test_mba$p.signif, ") than males at the MBA level"),
       fill = "Gender",
       x = NULL,
       y = "Score")+
  theme_few()

t_test_mba %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```


## Did the academic peformance help in receiving an offer? {.tabset .tabset-fade .tabset-pills}


Since we know that the results got more concentrated around the median (60-70) as the student progressed in their education, we could infer that there was less of a chance to differentiate themselves based on grade and more based on other factors such as:
* *employability score*
* *work experience*
* *technical skills*
* *soft skills*
* *better interviewing skills*

Unfortunately, out of these factors, we only have data on the employability score.


**Key findings**:

The score differences between those who received an offer and those who did not:

* were highly significant at the **secondary**, **higher secondary** and **university** level
* significant at **employability test**
* no significance at the **MBA** level


### secondary {-}


```{r}
t_test <- raw_data %>%
  t_test(ssc_p ~ status)%>%
  add_significance()

raw_data %>% 
  ggplot(aes(ssc_p, fill=status, col = status))+
  geom_density(alpha = 0.3, show.legend = FALSE, lwd = 1)+
  geom_rug()+
  scale_fill_manual(values = c("#DC3220", "#40B0A6"))+
  scale_colour_manual(values = c("#DC3220", "#40B0A6"))+
  labs(x = "Score",
       y = "Density",
       col = "Status")+
  theme_few()

t_test %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```


### higher secondary {-}


```{r}
t_test <- raw_data%>%
  t_test(hsc_p ~ status)%>%
  add_significance()

raw_data %>% 
  ggplot(aes(hsc_p, fill=status, col = status))+
  geom_density(alpha = 0.3, show.legend = FALSE, lwd = 1)+
  geom_rug()+
  scale_fill_manual(values = c("#DC3220", "#40B0A6"))+
  scale_colour_manual(values = c("#DC3220", "#40B0A6"))+
  labs(x = "Score",
       y = "Density",
       col = "Status")+
  theme_few()

t_test %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```


### university {-}


```{r results = "asis"}
t_test <- raw_data%>%
  t_test(degree_p ~ status)%>%
  add_significance()

raw_data %>% 
  ggplot(aes(degree_p, fill=status, col = status))+
  geom_density(alpha = 0.3, show.legend = FALSE, lwd = 1)+
  geom_rug()+
  scale_fill_manual(values = c("#DC3220", "#40B0A6"))+
  scale_colour_manual(values = c("#DC3220", "#40B0A6"))+
  labs(x = "Score",
       y = "Density",
       col = "Status")+
  theme_few()

t_test %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```


### MBA {-}


```{r results = "asis"}
t_test <- raw_data%>%
  t_test(mba_p ~ status)%>%
  add_significance()

raw_data %>% 
  ggplot(aes(mba_p, fill=status, col = status))+
  geom_density(alpha = 0.3, show.legend = FALSE, lwd = 1)+
  geom_rug()+
  scale_fill_manual(values = c("#DC3220", "#40B0A6"))+
  scale_colour_manual(values = c("#DC3220", "#40B0A6"))+
  labs(x = "Score",
       y = "Density",
       col = "Status")+
  theme_few()

t_test %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```


### employability (non-academic) {-}


```{r}
t_test <- raw_data%>%
  t_test(etest_p ~ status)%>%
  add_significance()

raw_data %>% 
  ggplot(aes(etest_p, fill=status, col = status))+
  geom_density(alpha = 0.3, show.legend = FALSE, lwd = 1)+
  geom_rug()+
  scale_fill_manual(values = c("#DC3220", "#40B0A6"))+
  scale_colour_manual(values = c("#DC3220", "#40B0A6"))+
  labs(x = "Score",
       y = "Density",
       col = "Status")+
  theme_few()

t_test %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```


## Did the different boards make a significant difference in school peformance/placement offer? {.tabset .tabset-fade .tabset-pills}


**Key findings**:

* no significant differences in school performance between the two boards at either **secondary** or **higher secondary** level
* there was also no significant difference in the number of people that received an offer from either board at the **secondary** or **higher secondary** level


### secondary {-}


```{r}
#vs performance
raw_data %>% 
  ggplot(aes(ssc_p, fill=ssc_b, col = ssc_b))+
  geom_density(alpha = 0.3, show.legend = FALSE, lwd = 1)+
  geom_rug()+
  scale_colour_manual(values = c("#00AFBB", "#E69F00"))+
  scale_fill_manual(values = c("#00AFBB", "#E69F00"))+
  labs(col = "Board",
       x = "Score",
       y = "Density")+
  theme_few()

#t-test
t_test <- raw_data%>%
  t_test(ssc_p ~ ssc_b)%>%
  add_significance()%>%
  kbl() %>%
  kable_paper("hover", full_width = F)

t_test

#vs placement
raw_data %>% 
  ggplot(aes(ssc_b, fill = status))+
  geom_bar(position = "fill", col = "black")+
  scale_fill_manual(values = c("#DC3220", "#40B0A6"))+
  scale_colour_manual(values = c("#DC3220", "#40B0A6"))+
  labs(fill = "Status",
       x = "Board",
       y = "Percentage of students")+
  scale_y_continuous(label = percent)+
  theme_few()

#categorical variables
chisq.test(table(raw_data$ssc_b, raw_data$status))
```


### higher secondary {-}


```{r}
#vs performance
raw_data %>% 
  ggplot(aes(hsc_p, fill=hsc_b, col = hsc_b))+
  geom_density(alpha = 0.3, show.legend = FALSE, lwd = 1)+
  geom_rug()+
  scale_colour_manual(values = c("#00AFBB", "#E69F00"))+
  scale_fill_manual(values = c("#00AFBB", "#E69F00"))+
  labs(col = "Board",
       x = "Score",
       y = "Density")+
  theme_few()

#t-test for performance
t_test <- raw_data%>%
  t_test(hsc_p ~ hsc_b)%>%
  add_significance()%>%
  kbl() %>%
  kable_paper("hover", full_width = F)

t_test

#vs placement
raw_data %>% 
  ggplot(aes(hsc_b, fill = status))+
  geom_bar(position = "fill", col = "black")+
  scale_fill_manual(values = c("#DC3220", "#40B0A6"))+
  scale_colour_manual(values = c("#DC3220", "#40B0A6"))+
  labs(fill = "Status",
       x = "Board",
       y = "Percentage of students")+
  scale_y_continuous(label = percent)+
  theme_few()

#categorical variables
chisq.test(table(raw_data$hsc_b, raw_data$status))
```


## Did the specialisation help in getting a role? {.tabset .tabset-fade .tabset-pills}


**Key findings**:

* **higher secondary**:
  * Commerce students scored significantly lower than Science students
  * no significant difference between any groups in placement offers (however, this might not be accurate given that there were only 11 students in Arts versus 113 in Commerce and 91 in Science)
* **university**:
  * Science and Technology students scored significantly higher than those in Others
  * there were no significant differences between the different groups regarding as it regarded the amount of students that received an offer (results might not be accurate due to a very low sample sample size in Others of 11 versus 145 in Commerce and Management and 59 in Science and Technology)
* **MBA**:
  * there were no significant differences in performance between the two groups
  * significantly more Marketing and Finance students received an offer when compared to those specialised in Marketing and HR


### higher secondary {-}


```{r}
#vs performance
raw_data %>% 
  ggplot(aes(hsc_p, hsc_s, fill=hsc_s, col = hsc_s))+
  geom_boxplot(alpha = 0.5, show.legend = FALSE, lwd = 1)+
  geom_rug()+
  scale_colour_manual(values = c("#00AFBB", "#E69F00", "blueviolet"))+
  scale_fill_manual(values = c("#00AFBB", "#E69F00", "blueviolet"))+
  labs(col = "Board",
       x = "Score",
       y = "Specialisation")+
  theme(legend.position = "none")+
  theme_few()

#t-test
t_test <- raw_data%>%
  t_test(hsc_p ~ hsc_s)%>%
  add_significance()%>%
  kbl() %>%
  kable_paper("hover", full_width = F)

t_test

#vs placement
raw_data %>% 
  ggplot(aes(hsc_s, fill = status))+
  geom_bar(position = "fill", col = "black")+
  scale_fill_manual(values = c("#DC3220", "#40B0A6"))+
  scale_colour_manual(values = c("#DC3220", "#40B0A6"))+
  labs(fill = "Status",
       x = "Specialisation",
       y = "Percentage of students")+
  scale_y_continuous(label = percent)+
  theme_few()

#categorical variables
chisq.test(table(raw_data$hsc_s, raw_data$status))
```


### university {-}


```{r}
#vs performance
raw_data %>% 
  ggplot(aes(degree_p, degree_t, fill=degree_t, col = degree_t))+
  geom_boxplot(alpha = 0.5, show.legend = FALSE, lwd = 1)+
  geom_rug()+
  scale_colour_manual(values = c("#00AFBB", "#E69F00", "blueviolet"))+
  scale_fill_manual(values = c("#00AFBB", "#E69F00", "blueviolet"))+
  labs(col = "Board",
       x = "Score",
       y = "Specialisation")+
  theme_few()

#t-test
t_test <- raw_data%>%
  t_test(degree_p ~ degree_t)%>%
  add_significance()%>%
  kbl() %>%
  kable_paper("hover", full_width = F)

t_test

#vs placement
raw_data %>% 
  ggplot(aes(degree_t, fill = status))+
  geom_bar(position = "fill", col = "black")+
  scale_fill_manual(values = c("#DC3220", "#40B0A6"))+
  scale_colour_manual(values = c("#DC3220", "#40B0A6"))+
  labs(fill = "Status",
       x = "Specialisation",
       y = "Percentage of students")+
  scale_y_continuous(label = percent)+
  theme_few()

#categorical variables
chisq.test(table(raw_data$degree_t, raw_data$status))
```


### MBA {-}


```{r}
#vs performance
raw_data %>% 
  ggplot(aes(mba_p, fill=specialisation, col = specialisation))+
  geom_boxplot(alpha = 0.3, show.legend = FALSE, lwd = 1)+
  geom_rug()+
  scale_colour_manual(values = c("#00AFBB", "#E69F00"))+
  scale_fill_manual(values = c("#00AFBB", "#E69F00"))+
  labs(col = "Specialisation",
       x = "Score",
       y = "Density")+
  theme_few()

#t-test
t_test <- raw_data%>%
  t_test(mba_p ~ specialisation)%>%
  add_significance()%>%
  kbl() %>%
  kable_paper("hover", full_width = F)

t_test

#vs placement
raw_data %>% 
  ggplot(aes(specialisation, fill = status))+
  geom_bar(position = "fill", col = "black")+
  scale_fill_manual(values = c("#DC3220", "#40B0A6"))+
  scale_colour_manual(values = c("#DC3220", "#40B0A6"))+
  labs(fill = "Status",
       x = "Specialisation",
       y = "Percentage of students")+
  scale_y_continuous(label = percent)+
  theme_few()

#categorical variables
chisq.test(table(raw_data$specialisation, raw_data$status))
```


## Is there a difference between the two genders in getting a placement?


**Key findings**:

* there were no significant differences in gender between the students that received a role and those that did not


```{r}
#graph
raw_data %>%
  group_by(gender, status)%>%
  summarise(count = n()/100)%>%
  mutate(percentage = round(count/sum(count),2))%>%
  
ggplot(aes(gender, count, fill = status))+
  geom_col(position = "fill", col = "black")+
  geom_label(aes(x=gender, 
                 y = ifelse(percentage>.5, percentage - .25, percentage + .5), 
                 label = paste(percentage*100, "%")), 
             show.legend = FALSE)+
  scale_fill_manual(values = c("#DC3220", "#40B0A6"))+
  scale_y_continuous(label = percent)+
  labs(x = "Gender",
       y = "% of students",
       fill = "Status",
       subtitle = "Proportionaly, more males were offered a role but this was not statistically significant")+
  theme_few()

#categorical variables
chisq.test(table(raw_data$gender, raw_data$status)) %>% print()
```


## Does work experience matter in getting a role?


**Key findings**:

* significantly more students with work experience received offers than those without any work experience


```{r}
#graph
raw_data %>% group_by(workex, status)%>% 
  summarise(count = n())%>%
  mutate(percentage = round(count / sum(count),2))%>%
        
  ggplot(aes(workex, percentage, fill = status))+
  geom_col(col = "black")+
  scale_fill_manual(values = c("#DC3220", "#40B0A6"))+
  geom_label(aes(x=workex, 
                 y = ifelse(percentage>.46, percentage - .25, 
                            ifelse(percentage>.65, -.20, 
                                   .92)), 
                 label = paste(percentage*100, "%")), 
             show.legend = FALSE)+
  labs(title = "Work experience significantly increases the chance of getting placed",
       subtitle = "87% of peple with work experience were placed versus 60% of those without work experience",
       x = "Work Experience",
       y = "% of students",
       fill = "Status")+
  theme_few()+
  theme(axis.ticks.y = element_blank(),
        axis.text.y = element_blank())

#categorical variables
chisq.test(table(raw_data$workex, raw_data$status)) %>% print()
```


# Classification


## Data split


Based on the previous Exploratory Data Analysis, we're not very clear on whether any of the features are insignificant, so none of them are to be removed from the modelling.

Let's split the dataset into the following:

* a train dataset of 80% of the raw data to train our prediction model on
* a test dataset of 20% of the raw data to then test it on


```{r}
set.seed(123)

#clean model data
raw_data_model <- raw_data %>%
  select(-salary)%>%
  mutate(status = as.factor(make.names(status)))

#split into two datasets
split <- createDataPartition(raw_data_model$status,
                             p =0.8, 
                             list = FALSE)

train_data <- raw_data_model[split,]
test_data <- raw_data_model[-split,]
```


## Pre-processing


Before we dive into the model building, we need to think about any pre-processing that might need doing.


### missing values {-}


We've seen before that there was no missing data to worry about, so we can skip that step.


### one hot encoding {-}


```{r eval=FALSE, include=FALSE}
#this feature plot might only work after hot encoding
featurePlot(x = train_data[,1:13], 
            y = train_data$status, 
            plot = "box",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))

#same but with density plots
featurePlot(x = train_data[,1:13], 
            y = train_data$status, 
            plot = "density",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))

#TO NOTE: these might show which variables are likely to be important, not which are NOT. Let's not exclude at this stage!
```


### range normalisation {-}


Let's use some basic standardisation offered by the caret package such as **centering** (subtract mean from values) and **scaling** (divide values by standard deviation).


```{r}
#store the ID variable in its original format
y_test <- test_data$sl_no
y_train <- train_data$sl_no

#take out y variable temporarily as we do not want this to be processed
test_data <- test_data %>% select (-sl_no)
train_data <- train_data %>% select (-sl_no)

#center and scale our data
preProcess_range_model <- preProcess(train_data, method=c("center", "scale"))

train_data <- predict(preProcess_range_model, newdata = train_data)
test_data <- predict(preProcess_range_model, newdata = test_data)
```


## Resampling


We will perform a 10-fold Cross Validation 5 times. This means we will be dividing the training dataset randomly into 10 parts, use each of the 10 parts as a "testing dataset" for the model and "train" on the remaining 9. 

Essentially, we are "pretending" that some of our data is new and use the rest of the data to model on. We then take the average error of each of the 10 models and repeat this process 5 times. Doing it more than once will give a more realistic sense of how the model will perform on new data.


```{r}
train.control <- trainControl(method = "repeatedcv", 
                              number = 10, #10
                              repeats = 5, #5
                              #search = "random",
                              classProbs = T)

tune.Length <- 10

clusters <- 4
```


## Model building {.tabset .tabset-fade .tabset-pills}

The data has a higher number of observations than features so we will be choosing appropriates algorithms  KNN, Decision trees, or kernel SVM). Let's build a few of these models and see how they compare.


### 1 - GLM {-}


```{r echo=TRUE, cache = TRUE, results='hide'}
#run them all in paralel
cl <- makeCluster(clusters, type = "SOCK")
 
#register cluster train in paralel
registerDoSNOW(cl)

#train model
model_glm <- train(status ~ .,
                   data = train_data,
                   method = "glm",
                   trControl = train.control,
                   tuneLength = tune.Length)

#shut the instances of R down
stopCluster(cl)

#show results
summary(model_glm)

#get info
#names(model_glm)

#AccuracySD
model_glm$results #gives us an esitmate of the uncertainty in our accuracy estimate

#add prediction column to test dataset
test_data$glm <- predict(model_glm, newdata = test_data)

#get probabilities instead
head(predict(model_glm, newdata = test_data, type = "prob"))
```


```{r}
preds <- predict(model_glm, newdata = test_data)

confusionMatrix(preds, test_data$status)
```


### 2 - Decision tree {-}


```{r echo=TRUE, cache = TRUE, results='hide'}
#run them all in paralel
cl <- makeCluster(clusters, type = "SOCK")
 
#register cluster train in paralel
registerDoSNOW(cl)

#train model
model_rpart <- train(status ~ .,
                      data = train_data,
                      method = "rpart",
                      trControl = train.control,
                      tuneLength = tune.Length)

#shut the instances of R down
stopCluster(cl)

#show results
summary(model_rpart)

#get info
#names(model_rpart)

#AccuracySD
model_rpart$results #gives us an esitmate of the uncertainty in our accuracy estimate

#add prediction column to test dataset
test_data$rpart <- predict(model_rpart, newdata = test_data)

#get probabilities instead
head(predict(model_rpart, newdata = test_data, type = "prob"))
```


```{r}
preds <- predict(model_rpart, newdata = test_data)

confusionMatrix(preds, test_data$status)
```


### 3 - LDA2 {-}


```{r echo=TRUE, cache = TRUE, results='hide'}
#run them all in paralel
cl <- makeCluster(clusters, type = "SOCK")
 
#register cluster train in paralel
registerDoSNOW(cl)

#train model
model_lda2 <- train(status ~ .,
                        data = train_data,
                        method = "lda2",
                        trControl = train.control,
                        tuneLength = tune.Length)

#shut the instances of R down
stopCluster(cl)

#show results
summary(model_lda2)

#get info
#names(model_lda2)

#AccuracySD
model_lda2$results #gives us an esitmate of the uncertainty in our accuracy estimate

#add prediction column to test dataset
test_data$lda2 <- predict(model_lda2, newdata = test_data)

#get probabilities instead
head(predict(model_lda2, newdata = test_data, type = "prob"))
```


```{r}
preds <- predict(model_lda2, newdata = test_data)

confusionMatrix(preds, test_data$status)
```


### 4 - SVM Linear {-}


```{r echo=TRUE, cache = TRUE, results='hide'}
#run them all in paralel
cl <- makeCluster(clusters, type = "SOCK")
 
#register cluster train in paralel
registerDoSNOW(cl)

#train model
model_svm <- train(status ~ .,
                   data = train_data,
                   method = "svmLinear",
                   trControl = train.control,
                   tuneLength = tune.Length)

#shut the instances of R down
stopCluster(cl)

#show results
summary(model_svm)

#get info
#names(model_svm)

#AccuracySD
model_svm$results #gives us an esitmate of the uncertainty in our accuracy estimate

#add prediction column to test dataset
test_data$svm <- predict(model_svm, newdata = test_data)

#get probabilities instead
head(predict(model_svm, newdata = test_data, type = "prob"))
```


```{r}
preds <- predict(model_svm, newdata = test_data)

confusionMatrix(preds, test_data$status)
```


### 5 - xgboost {-}


```{r eval=FALSE, cache=TRUE, include=FALSE, results='hide'}
#run them all in paralel
cl <- makeCluster(clusters, type = "SOCK")

#register cluster train in paralel
registerDoSNOW(cl)

#train model
model_xgb <- train(status ~ .,
                   data = train_data,
                   method = "xgbTree",
                   trControl = train.control,
                   tuneLength = tune.Length)

#shut the instances of R down
stopCluster(cl)

#show results
summary(model_xgb)

#get info
#names(model_xgb)

#AccuracySD
model_xgb$results #gives us an esitmate of the uncertainty in our accuracy estimate

#add prediction column to test dataset
test_data$xgb <- predict(model_xgb, newdata = test_data)

#get probabilities instead
head(predict(model_xgb, newdata = test_data, type = "prob"))
```


```{r eval=FALSE, include=FALSE}
preds <- predict(model_xgb, newdata = test_data)

confusionMatrix(preds, test_data$status)
```


### 6 - KNN {-}


```{r echo=TRUE, cache = TRUE, results='hide'}
#run them all in paralel
cl <- makeCluster(clusters, type = "SOCK")

#register cluster train in paralel
registerDoSNOW(cl)

#train model
model_knn <- train(status ~ .,
                   data = train_data,
                   method = "knn",
                   trControl = train.control,
                   tuneLength = tune.Length)

#shut the instances of R down
stopCluster(cl)

#show results
summary(model_knn)

#get info
#names(model_knn)

#AccuracySD
model_knn$results #gives us an esitmate of the uncertainty in our accuracy estimate

#add prediction column to test dataset
test_data$knn <- predict(model_knn, newdata = test_data)

#get probabilities instead
head(predict(model_knn, newdata = test_data, type = "prob"))
```


```{r}
preds <- predict(model_knn, newdata = test_data)

confusionMatrix(preds, test_data$status)
```


### 7 - Random Forrest (ranger) {-}


```{r echo=TRUE, cache = TRUE, results='hide'}
#run them all in paralel
cl <- makeCluster(clusters, type = "SOCK")

#register cluster train in paralel
registerDoSNOW(cl)

#train model
model_ranger <- train(status ~ .,
                  data = train_data,
                  method = "ranger",
                  trControl = train.control,
                  tuneLength = tune.Length)

#shut the instances of R down
stopCluster(cl)

#show results
summary(model_ranger)

#get info
#names(model_ranger)

#AccuracySD
model_ranger$results #gives us an esitmate of the uncertainty in our accuracy estimate

#add prediction column to test dataset
test_data$ranger <- predict(model_ranger, newdata = test_data)

#get probabilities instead
#head(predict(model_ranger, newdata = test_data, type = "prob"))


###----------------------------------why can we not predict?
```


```{r}
preds <- predict(model_ranger, newdata = test_data)

confusionMatrix(preds, test_data$status)
```


### 8 - Extreme Gradient Boosting {-}


```{r eval=FALSE, cache=TRUE, include=FALSE, results='hide'}
#run them all in paralel
cl <- makeCluster(clusters, type = "SOCK")

#register cluster train in paralel
registerDoSNOW(cl)

#train model
model_xgbDART <- train(status ~ .,
                       data = train_data,
                       method = "xgbDART",
                       trControl = train.control,
                       tuneLength = tune.Length)

#shut the instances of R down
stopCluster(cl)

#show results
summary(model_xgbDART)

#get info
#names(model_xgbDART)

#AccuracySD
model_xgbDART$results #gives us an esitmate of the uncertainty in our accuracy estimate

#add prediction column to test dataset
test_data$xgbDART <- predict(model_xgbDART, newdata = test_data)

#get probabilities instead
head(predict(model_xgbDART, newdata = test_data, type = "prob"))
```


```{r eval=FALSE, include=FALSE}
preds <- predict(model_xgbDART, newdata = test_data)

confusionMatrix(preds, test_data$status)
```


### 9 - Random Forrest (rf) {-}


```{r echo=TRUE, cache = TRUE, results='hide'}
#run them all in paralel
cl <- makeCluster(clusters, type = "SOCK")

#register cluster train in paralel
registerDoSNOW(cl)

#train model
model_rf <- train(status ~ .,
                   data = train_data,
                   method = "rf",
                   trControl = train.control,
                   tuneLength = tune.Length)

#shut the instances of R down
stopCluster(cl)

#show results
summary(model_rf)

#get info
#names(model_rf)

#AccuracySD
model_rf$results #gives us an esitmate of the uncertainty in our accuracy estimate

#add prediction column to test dataset
test_data$rf <- predict(model_rf, newdata = test_data)

#get probabilities instead
head(predict(model_rf, newdata = test_data, type = "prob"))
```


```{r}
preds <- predict(model_rf, newdata = test_data)

confusionMatrix(preds, test_data$status)
```


### 10 - MARS {-}


```{r echo=TRUE, cache = TRUE, results='hide'}
#run them all in paralel
cl <- makeCluster(clusters, type = "SOCK")

#register cluster train in paralel
registerDoSNOW(cl)

#train model
model_mars <- train(status ~ .,
                    data = train_data,
                    method = "earth",
                    trControl = train.control,
                    tuneLength = tune.Length)

#shut the instances of R down
stopCluster(cl)

#show results
summary(model_mars)

#get info
#names(model_mars)

#AccuracySD
model_mars$results #gives us an esitmate of the uncertainty in our accuracy estimate

#add prediction column to test dataset
test_data$mars <- predict(model_mars, newdata = test_data)

#get probabilities instead
head(predict(model_mars, newdata = test_data, type = "prob"))
```


```{r}
preds <- predict(model_mars, newdata = test_data)

confusionMatrix(preds, test_data$status)
```


## Which was the best model?


```{r}
#create a vector list
models_compare <- resamples(list(glm=model_glm,
                                 lda2 = model_lda2,
                                 rpart = model_rpart,
                                 ranger = model_ranger,
                                 svm = model_svm,
                                 #xgb = model_xgb,
                                 knn = model_knn,
                                 #xgbDART = model_xgbDART,
                                 rf = model_rf,
                                 mars = model_mars))

#create a scales vector
scales <- list(x=list(relation="free"), y=list(relation="free"))

#draw a box to compare models
bwplot(models_compare, scales=scales)
```


```{r}
# Summary of the models performances
summary(models_compare)

#The xgbDART model appears to be the be best performing model overall because of the high ROC. But if you need a model that predicts the positives better, you might want to consider MARS, given its high sensitivity.
```


We also need to consider that while the SVM model was the most accurate, there is a difference between the type of error that a model can make. In other words, predicting that one person will get the job when they ended up not getting the job is not as bad as predicting that a person won't get the job when they will end up getting the job.

a) if the first case is seen as favourable, the SVM model is more appropriate because it has a higher overall accuracy as it cause one false positive and one false positive
b) if the second is favourable, then the Random Forrest might be a better alternative. While it had a lower overall accuracy, it did not cause any X errors


## LDA2 vs Ranger


We can observe that there are statistically significant differences between these two main models.


```{r}
compare_models(model_lda2, model_ranger)
```


## Predictions table


We can see the actual predictions of each model below.


```{r}
test_data%>%
  select(status, lda2, ranger, svm, glm, rf, knn, mars, rpart)%>% #ensure these are filled in
  rename(Actual = status)%>%
  mutate(lda2 = ifelse(Actual == lda2, 
                      cell_spec(lda2, "html", color = "limegreen", bold = T), 
                      cell_spec(lda2, "html", color = "red", bold = T)),
         ranger = ifelse(Actual == ranger, 
                      cell_spec(ranger, "html", color = "limegreen", bold = T), 
                      cell_spec(ranger, "html", color = "red", bold = T)),
         svm = ifelse(Actual == svm, 
                      cell_spec(svm, "html", color = "limegreen", bold = T), 
                      cell_spec(svm, "html", color = "red", bold = T)),
         glm = ifelse(Actual == glm, 
                      cell_spec(glm, "html", color = "limegreen", bold = T), 
                      cell_spec(glm, "html", color = "red", bold = T)),
         #xgb = ifelse(Actual == xgb, 
         #             cell_spec(xgb, "html", color = "limegreen", bold = T), 
         #             cell_spec(xgb, "html", color = "red", bold = T)),
         rf = ifelse(Actual == rf, 
                      cell_spec(rf, "html", color = "limegreen", bold = T), 
                      cell_spec(rf, "html", color = "red", bold = T)),
         knn = ifelse(Actual == knn, 
                      cell_spec(knn, "html", color = "limegreen", bold = T), 
                      cell_spec(knn, "html", color = "red", bold = T)),
         mars = ifelse(Actual == mars, 
                      cell_spec(mars, "html", color = "limegreen", bold = T), 
                      cell_spec(mars, "html", color = "red", bold = T)),
         #xgbDART = ifelse(Actual == xgbDART, 
         #             cell_spec(xgbDART, "html", color = "limegreen", bold = T), 
        #              cell_spec(xgbDART, "html", color = "red", bold = T)), 
         rpart = ifelse(Actual == rpart, 
                      cell_spec(rpart, "html", color = "limegreen", bold = T), 
                      cell_spec(rpart, "html", color = "red", bold = T))) %>%
  
  kbl(escape = FALSE,
    caption = "Actual performance vs prediction (in order of descending accuracy performance)")%>%
  kable_paper(c("hover", "striped"), full_width = F)%>%
  column_spec(1, bold = T, color = "black")%>%
  scroll_box(height = "400px")
```


Two interesting things stand out here:

* whenever the LDA2 algorithm made a wrong prediction on someone who did not actually get a role, all others models made the same prediction. In simple terms, ALL algorithms said "these people should've received a role based on the data" but did not. This could be due to a multitude of factors that were not captured here
* the most accurate model had one wrong prediction on a student who did get a role, which was predicted not to (we'd like to avoid this as possible in order to not discourage students from applying to a role they're likely to get)

We should investigate to see what's special about these two groups of students.


### Why do the models agree that 3 students should've gotten the role?


Let's graph these students against the top 2-3 variables by importance and see if there are any trends

```{r}
#Append the Y variable back on with original values
test_data$sl_no <- y_test  
train_data$sl_no <- y_train

#clear variables from memory
rm(y_test, y_train)
```


```{r eval=FALSE, include=FALSE}
#get student's id
test_data%>%
  filter(ranger!=status & status == "Not.Placed")%>%
  select(sl_no, ssc_p, hsc_p)%>%
  print()

#see original data
raw_data %>% filter() %>%
  print()

#graph it
raw_data %>% mutate(class = ifelse(sl_no %in% c(10,76, 98, 131, 142, 159, 166, 182), "Yes", "No"),
                    size = as.factor(ifelse(sl_no %in% c(10,76, 98, 131, 142, 159, 166, 182), 5, 4)))%>%
  
  ggplot(aes(ssc_p, hsc_p, col = status, size = size))+
  geom_point()+
  geom_vline(xintercept =69.8)+
  geom_hline(yintercept =60.8)+
  facet_wrap(workex~specialisation)
```


### Why do the most accurate models say that one person shouldn't have gotten the role?


```{r eval=FALSE, include=FALSE}
#get student's id
test_data%>%
  filter(lda2!=status & status == "Placed")%>%
  select(sl_no, ssc_p, hsc_p)%>%
  print()

#see original data
raw_data %>% filter(sl_no == 23) %>%
  print()

#graph it
raw_data %>% mutate(class = ifelse(sl_no == 23, "Yes", "No"),
                    size = as.factor(ifelse(sl_no == 23, 5, 4)))%>%
  
  ggplot(aes(mba_p, degree_p, col = status, size = size))+
  geom_point()+
  geom_vline(xintercept =69.8)+
  geom_hline(yintercept =60.8)+
  facet_wrap(workex~specialisation)

```


It is possible that these people looked right on paper, but had major flaws in their interviewing skills or other similar factors that were not captured in the data. If that's the case, they should improve on those specific issues.


What was the probability of each of these? Was it 1-99 or 45-55?


## How many predictors did the most optimal model have?


Picking the lowest number of predictors is not always the wisest: they might've not had the chance to show their worth (low sample) or even due to the fact that we already know a specific factor is important. The number of predictors was calculated based on the Random Forrest (rf) Model.


```{r predictors}
plot(model_rf, 
     main = "The most optimal model was that with 9 predictors")
```


## What were the most important variables?


```{r factors, fig.height=5.5}
imp <- varImp(model_rf) #can also use glm here

plot(imp, main = "Top variables ranked by importance",
     sub = "As calculated by the Random Forrest (rf) model")
```


# Summary EDA:

* grades became more concentrated around the median from secondary to MBA
* females performed significantly better than males during university and mba
* a higher proportion of males were placed in a role than females
* the students who received a role performed significantly better during secondary, highschool and university, not just MBA, irrespective of gender


# Summary algorithm:

* two algorithms seemed to outperform the others, depending on the purpose of the prediction
  * if we're trying to ensure the highest accuracy, the SVM model was the most accurate
  * if we're trying to minimise the false positive (ensure this is correct, to minimise the number of people that are predicted to NOT get the job when they would've otherwise got the job), the Random Forrest model was the best choice


# Next steps: 

* predictive algorithm on salary and which factors contributed








1) cost vs profit matrix (put in a nice table) - page 260


Observed
Response     Non response
TP             FP              26.4       2      Response
FN             TN              28.4       -      Non-response



2) describe the issue then of how this applies to the models that we have

3) draw a ROC

4) combine proportion of the model

* read others' project and learn from them

* what do the probability estimates for each class tell us about the model's confidence? (51% vs 99%) - equivocal areas

* try and understand why it failed on those people that the model failed on

- metric = "ROC"?
